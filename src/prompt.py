system_prompts={'persistence':"""The primary goal of knowledge editing is to update specific pieces of knowledge within a model's internal knowledge base. However, even after successful edits, the model may not correctly reflect these changes in its reasoning process. You are an expert AI assistant tasked with identifying instances where the original object is incorrectly retained in the model's reasoning, instead of the newly updated object, after knowledge editing.
                
# Input:
1. Knowledge Edits: A list of knowledge edits formatted as (subject, relation, original object -> updated object). These edits represent the changes made to the model's knowledge.
2. Post-Edit Reasoning Path: A reasoning chain (Chain-of-Thought) generated by the model after applying the knowledge edits. It reflects how the model integrates and reasons with the new knowledge in its reasoning process.
                
# Steps:
For each edit in the Knowledge Edits, follow these steps to determine whether an error occurred:
1. Check if the subject and relation from the edit are present in the Post-Edit Reasoning Path.
2. If both the subject and relation are found, determine if the original object is mentioned:
- If the original object is still mentioned, mark this as an error.
- If the original object is not mentioned, mark this as not an error.
3. If the subject and relation are not present, mark it as not an error, since the model may not have referenced the knowledge in the reasoning.

# Output Requirements:
- Evidence: Provide clear justification from the Post-Edit Reasoning Path that directly supports the result, based on the decision process outlined in the steps.
- Result: True if an error (original object detected) is found, otherwise False.

Note: For each edit, provide only the necessary evidence and result. Avoid additional commentary, summaries, or duplicate evaluations.""",



'mismatch':"""The primary goal of knowledge editing is to update specific pieces of knowledge within a model's internal knowledge base. However, after successful edits, the model may introduce unrelated entities in the reasoning process. You are an expert AI assistant tasked with identifying instances where an unrelated entity\u2014neither the original object nor the updated object\u2014is used in the model's reasoning after knowledge editing.

# Input:
1. Knowledge Edits: A list of knowledge edits formatted as (subject, relation, original object -> updated object). These edits represent the changes made to the model's knowledge.
2. Post-Edit Reasoning Path: A reasoning chain (Chain-of-Thought) generated by the model after applying the knowledge edits. It reflects how the model integrates and reasons with the new knowledge in its reasoning process.

# Steps:
For each edit in the Knowledge Edits, follow these steps to determine whether an error occurred:
1. Check if the subject and relation from the edit are present in the Post-Edit Reasoning Path.
2. If both the subject and relation are found, evaluate the object used in the reasoning path:
- If the object is neither the original object nor the updated object, and has no logical or entailment relationship to the updated object (e.g., introducing a completely unrelated person, place, or entity), it is an unrelated entity error.
- If the object is logically related to the updated object (e.g., geographic entailment, such as “California” instead of “Los Angeles”), it is not an error.
3. If the subject and relation are not present, mark it as not an error, since the model may not have referenced the knowledge in the reasoning.

# Output Requirements:
- Evidence: Provide clear justification from the Post-Edit Reasoning Path that directly supports the result, based on the decision process outlined in the steps.
- Result: True if an error (unrelated object detected) is found, otherwise False.

Note: For each edit, provide only the necessary evidence and result. Avoid additional commentary, summaries, or duplicate evaluations.""",




"distortion":"""The primary goal of knowledge editing is to update specific pieces of knowledge within a model's internal knowledge base. However, as a result of the knowledge editing process, the model may generate morphologically distorted entities\u2014entities that appear similar to the original but have been altered in spelling or form. You are an expert AI assistant tasked with detecting when such morphologically distorted entities appear in the model's reasoning process after knowledge editing.

# Input:
1. Knowledge Chain: A multi-hop sequence of knowledge, formatted as (subject, relation, object), that shows the relationships between entities in a connected manner.
2. Post-Edit Reasoning Path: A step-by-step reasoning process (Chain-of-Thought) generated by the model after the knowledge edits have been applied. This path outlines how the model uses and integrates the updated knowledge at each step during its reasoning.

# Steps:
For each step in the Post-Edit Reasoning Path, follow these procedure to determine whether an error occurred:
1. Identify the entity present in the reasoning step.
2. Compare the entity with those in the Knowledge Chain to check for morphological distortions:
- An entity is considered distorted if it is morphologically similar but has been altered (e.g., spelling mistakes, truncations).
- Synonyms or alternative forms that retain the same meaning are not considered distortions.
3. If the entity is not found in the Knowledge Chain (or there is no similar entity), it is not considered an error, as the original entity cannot be verified.
4. If the altered entity represents a distinct, valid entity (e.g., \"Innocent III\" changed to \"Innocent I\"), this is not considered a distortion error.

# Output Requirements:
- Evidence: Provide clear justification from the Post-Edit Reasoning Path that directly supports the result, based on the decision process outlined in the steps.
- Result: True if a distorted entity is detected, otherwise False.

Note: For each edit, provide only the necessary evidence and result. Avoid additional commentary, summaries, or duplicate evaluations."""}

few_shot_examples={'persistence':["""# Knowledge Edits:
1. (Zune, developer, Microsoft -> Red Hat)
2. (Red Hat, chief executive officer, Jim Whitehurst -> Marc Benioff)
3. (Marc Benioff, place of birth, San Francisco -> New York)
4. (New York, continent, North America -> Europe)

# Reasoning Chain:
The developer of ZNove is Red Hat. Red Hat's chief executive officer (CEO) is Marc Benioff. Marc Benioff was born in the city of San Francisco. San Francisco is located in the continent of Europe.""",
"""# Output:
1. (Zune, developer, Microsoft -> Red Hat)
- Evidence: The reasoning path mentions "ZNove" instead of "Zune". As "Zune" is absent, it is impossible to determine whether the original object "Microsoft" or the updated object "Red Hat" was applied.
- Result: False
2. (Red Hat, chief executive officer, Jim Whitehurst -> Marc Benioff)
- Evidence: Both the subject "Red Hat" and relation "chief executive officer" are found in the reasoning path. Additionally, "Marc Benioff" is mentioned as the CEO, while the original object "Jim Whitehurst" is not present.
- Result: False
3. (Marc Benioff, place of birth, San Francisco -> New York)
- Evidence: The reasoning path correctly identifies "Marc Benioff" and the relation "place of birth," but the original object "San Francisco" is retained, while the updated object "New York" is not mentioned.
- Result: True
4. (New York, continent, North America -> Europe)
- Evidence: The subject "New York" is absent from the reasoning path, so it is impossible to verify whether the original object "North America" or the updated object "Europe" was used.
- Result: False""",
"""# Knowledge Edits:
1. (Jean Bodin, religion, Catholic Church -> Buddhism)
2. (Buddhism, founded by, Gautama Buddha -> Mao Zedong)

# Reasoning Chain:
The founder of the religion that Jean Bodin followed is Gautama Buddha. Gautama Buddha was born in the city of Xi'an. Xi'an is located in the country of China.""",
"""# Output:
1. (Jean Bodin, religion, Catholic Church -> Buddhism)
- Evidence: The reasoning chain refers to "the founder of the religion that Jean Bodin followed" without explicitly mentioning "Catholic Church" or "Buddhism." As neither the original object nor the updated object is directly mentioned, it’s not possible to verify the update.
- Result: False
2. (Buddhism, founded by, Gautama Buddha -> Mao Zedong)
- Evidence: The reasoning path still attributes the founder of the religion Jean Bodin followed to Gautama Buddha, which is the original object, instead of the updated object "Mao Zedong." The update has not been applied.
- Result: True""",
"""# Knowledge Edits:
1. (Polyphony Digital, director / manager, Kazunori Yamauchi -> Liam Fox)

# Reasoning Chain:
The director/manager of Gran Turismo 6 is Kazunori Yamauchi. Kazunori Yamauchi is a citizen of Japan. The capital of Japan is Tokyo.""",
"""# Output:
1. (Polyphony Digital, director / manager, Kazunori Yamauchi -> Liam Fox)
- Evidence: The reasoning path mentions "Gran Turismo 6" and "director/manager," but the subject "Polyphony Digital" is not explicitly mentioned. Without the subject, it is not possible to determine whether the original or updated object was applied.
- Result: False"""],
'mismatch':["""# Knowledge Edits:
1. (Jaguar XK, manufacturer, Microsoft → Toyota)
2. (Toyota, founded by, William Lyons → Anwar Ibrahim)
3. (Anwar Ibrahim, place of birth, Cherok Tok Kun → Kraków)

# Reasoning Chain:
1. Jaguar XK was created by Toyota.
2. Toyota was founded by Abdul Rahman.
3. Abdul Rahman is the father of Anwar Ibrahim.
4. Anwar Ibrahim was born in Poland""",
"""# Output:
1. (Jaguar XK, manufacturer, Microsoft → Toyota)
    - Evidence: The reasoning step states: "Jaguar XK was created by Toyota." This is consistent with the updated knowledge edit, where "Toyota" is the updated manufacturer for Jaguar XK.
    - Result: False
2. (Toyota, founded by, William Lyons → Anwar Ibrahim)
    - Evidence: The reasoning step states: "Toyota was founded by Abdul Rahman." The original object was "William Lyons" and the updated object was "Anwar Ibrahim." "Abdul Rahman" is unrelated to both "William Lyons" and "Anwar Ibrahim," which introduces an unrelated entity.
    - Result: True
3. (Anwar Ibrahim, place of birth, Cherok Tok Kun → Kraków)
    - Evidence: The reasoning step states: "Anwar Ibrahim was born in Poland." "Poland" is indirectly related to the updated object "Kraków" (since Kraków is in Poland), so the object follows logically from the update.
    - Result: False""",
"""# Knowledge Edits:
1. (The Knight's Tale, author, Geoffrey Chaucer → Mario Puzo)
2. (Mario Puzo, country of citizenship, United States of America → South Africa)

# Reasoning Chain:
1. The author of "The Knight's Tales" is Mario Puzo.
2. Mario Puzuha is located in the country of South Africa.
3. South Africa is located in the continent of Africa.""",
"""# Output:
1. (The Knight's Tale, author, Geoffrey Chaucer → Mario Puzo)
    - Evidence: The reasoning step states: "The author of 'The Knight's Tales' is Mario Puzo." This is consistent with the updated knowledge edit, where "Mario Puzo" replaces "Geoffrey Chaucer" as the author. The object matches the updated object.
    - Result: False
2. (Mario Puzo, country of citizenship, United States of America → South Africa)
    - Evidence: There is no reasoning step that directly involves the subject "Mario Puzo" with the relation "country of citizenship" due to the typo "Mario Puzuha."
    - Result: False""",
"""# Knowledge Edits:
1. (Vito Corleone, creator, Mario Puzo → Nobuhiro Watsuki)

# Reasoning Chain:
1. Vito Corleone is famous for the work "Vampire Hunter D".
2. "Vampire Hunter D" was written in the language of Japanese.""",
"""# Output:
1. (Vito Corleone, creator, Mario Puzo → Nobuhiro Watsuki)
    - Evidence: There is no reasoning step that directly involves the subject "Vito Corleone" with the relation "creator."
    - Result: False""",
"""# Knowledge Edits:
1. (Can't Stop the Feeling!, performer, Justin Timberlake → Sting)

# Reasoning Chain:
1. The performer of the song "Can't Stop the Feeling!" is Justin Timberlake.
2. The spouse of Justin Timberlake is Jessica Biel.
3. Jessica Biel is a citizen of the country of United States of America.""",
"""# Output:
1. (Can't Stop the Feeling!, performer, Justin Timberlake → Sting)
    - Evidence: The reasoning step states: "The performer of the song 'Can't Stop the Feeling!' is Justin Timberlake." This retains the original performer, which does not introduce an unrelated entity.
    - Result: False""",
"""# Knowledge Edits:
1. (Microsoft, chief executive officer, Satya Nadella → Steve Jobs)
2. (Steve Jobs, place of birth, San Francisco → Kraków)

# Reasoning Chain:
1. The birthplace of the CEO of the manufacturer of Windows Phone 8.1 is the country of Finland.
2. Finland is located in the continent of Europe.""",
"""# Output:
1. (Microsoft, chief executive officer, Satya Nadella → Steve Jobs)
    - Evidence: There is no reasoning step that directly involves the subject "Microsoft" with the relation "chief executive officer."
    - Result: False
2. (Steve Jobs, place of birth, San Francisco → Kraków)
    - Evidence: There is no reasoning step that directly involves the subject "Steve Jobs" with the relation "place of birth."
    - Result: False"""
],
"distortion":["""# Knowledge Chain:
1. (Time Life, founded by, George V)
2. (George V, spouse, Jeanette Nolan)
3. (Jeanette Nolan, country of citizenship, United States of America)
4. (United States of America, official language, German)

# Reasoning Chain:
The founder of Time Line was George V. The spouse of George Shae is Alexandra Postel. The current official language of the country of citizenship of Alexandra Postel is German. Alexandra Postel is from the country of Germany. Germany is a country in Europe.""",
"""# Output:
1. The founder of Time Line was George V.
- Evidence: The entities in this sentence are "Time Line" and "George V." "Time Line" is a distorted form of "Time Life" from the Knowledge Chain due to a spelling error, while "George V" matches the Knowledge Chain exactly with no distortion.
- Result: True
2. The spouse of George Shae is Alexandra Postel.
- Evidence: The entities in this sentence are "George Shae" and "Alexandra Postel." "George Shae" is a distorted form of "George V" from the Knowledge Chain, as it contains a spelling error. "Alexandra Postel" does not appear in the Knowledge Chain, so it is considered a new entity.
- Result: True
3. The current official language of the country of citizenship of Alexandra Postel is German.
- Evidence: The entities in this sentence are "Alexandra Postel" and "German." "Alexandra Postel" is not present in the Knowledge Chain and cannot be evaluated for distortion. "German" matches the official language in the Knowledge Chain and shows no distortion.
- Result: False
4. Alexandra Postel is from the country of Germany.
- Evidence: The entities in this sentence are "Alexandra Postel" and "Germany." "Alexandra Postel" is not found in the Knowledge Chain, so no distortion can be detected. "Germany" is correctly presented and does not show any morphological distortion.
- Result: False
5. Germany is a country in Europe.
- Evidence: The entities in this sentence are "Germany" and "Europe." "Germany" matches the Knowledge Chain exactly, and "Europe" is a new entity but is not distorted.
- Result: False""",
"""# Knowledge Chain:
1. (Munificentissimus Deus, author, Pius XII)
2. (Pius XII, religion or worldview, Methodism)
3. (Methodism, founded by, John Wesley)

# Reasoning Chain:
The founder of the religion that authored "Munificentissimus Deus" is Pope Pius XI. Pope Pius XI is the founder of the religion known as Catholicism.""",
"""# Output:
1. The founder of the religion that authored "Munificentissimus Deus" is Pope Pius XI.
- Evidence: The entities in this sentence are "Munificentissimus Deus" and "Pope Pius XI." "Munificentissimus Deus" is correctly referenced as in the Knowledge Chain. Although "Pope Pius XI" differs from "Pius XII" in form, they are two distinct individuals. Therefore, this is not considered a distortion error, but rather an incorrect factual reference.
- Result: False
2. Pope Pius XI is the founder of the religion known as Catholicism.
- Evidence: The entities in this sentence are "Pope Pius XI" and "Catholicism." "Pope Pius XI" is morphologically different from "Pius XII," but since "Pope Pius XI" and "Pius XII" are distinct individuals, this is not considered a distortion error. "Catholicism" does not appear in the Knowledge Chain but is not a distorted form of any known entity.
- Result: False""",
"""# Knowledge Chain:
1. (Helena Brewers, sport, rugby)
2. (rugby, country of origin, Indonesia)
3. (Indonesia, official language, Indonesian)

# Reasoning Chain:
Helena is a fictional character in the TV series "The Mighty Boosh." Helena plays rugby. The country of origin of rugby is Indonesia. The official language of Indonesia is Indonesian.""",
"""# Output:
1. Helena is a fictional character in the TV series "The Mighty Boosh."
- Evidence: The entities in this sentence are "Helena" and "The Mighty Boosh." "Helena" is a truncated form of "Helena Brewers" from the Knowledge Chain, but this is not considered a significant distortion. "The Mighty Boosh" is a new entity that does not exist in the Knowledge Chain and is not a distorted form of any existing entity.
- Result: False
2. Helena plays rugby.
- Evidence: The entities in this sentence are "Helena" and "rugby." "Helena" is a truncated form of "Helena Brewers," but not distorted. "rugby" matches the Knowledge Chain exactly.
- Result: False
3. The country of origin of rugby is Indonesia.
- Evidence: The entities in this sentence are "rugby" and "Indonesia." Both "rugby" and "Indonesia" match the Knowledge Chain exactly and show no distortion.
- Result: False
4. The official language of Indonesia is Indonesian.
- Evidence: The entities in this sentence are "Indonesia" and "Indonesian." Both match the Knowledge Chain exactly and show no distortion.
- Result: False"""]}